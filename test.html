

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Single node on-prem deployment with vLLM or TGI on Xeon Scalable processors &mdash; TestOp 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="_static/documentation_options.js?v=2709fde1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Build MegaService of CodeGen on Xeon" href="usage.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            TestOp
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="usage.html">Build MegaService of CodeGen on Xeon</a><ul>
<li class="toctree-l2"><a class="reference internal" href="usage.html#create-an-aws-xeon-instance">🚀 Create an AWS Xeon Instance</a></li>
<li class="toctree-l2"><a class="reference internal" href="usage.html#build-docker-images">🚀 Build Docker Images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="usage.html#git-clone-genaicomps">1. Git Clone GenAIComps</a></li>
<li class="toctree-l3"><a class="reference internal" href="usage.html#build-the-llm-docker-image">2. Build the LLM Docker Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="usage.html#build-the-megaservice-docker-image">3. Build the MegaService Docker Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="usage.html#build-the-ui-docker-image">4. Build the UI Docker Image</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="usage.html#start-microservices-and-megaservice">🚀 Start Microservices and MegaService</a><ul>
<li class="toctree-l3"><a class="reference internal" href="usage.html#setup-environment-variables">Setup Environment Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="usage.html#start-the-docker-containers-for-all-services">Start the Docker Containers for All Services</a></li>
<li class="toctree-l3"><a class="reference internal" href="usage.html#validate-the-microservices-and-megaservice">Validate the MicroServices and MegaService</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="usage.html#launch-the-ui">🚀 Launch the UI</a></li>
<li class="toctree-l2"><a class="reference internal" href="usage.html#launch-the-react-based-ui-optional">🚀 Launch the React Based UI (Optional)</a></li>
<li class="toctree-l2"><a class="reference internal" href="usage.html#install-copilot-vscode-extension-from-plugin-marketplace-as-the-frontend">Install Copilot VSCode extension from Plugin Marketplace as the frontend</a><ul>
<li class="toctree-l3"><a class="reference internal" href="usage.html#how-to-use">How to Use</a></li>
<li class="toctree-l3"><a class="reference internal" href="usage.html#chat-with-ai-assistant">Chat with AI assistant</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Single node on-prem deployment with vLLM or TGI on Xeon Scalable processors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-building-pulling-docker-images">Prepare (Building / Pulling) Docker images</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#build-pull-microservice-images">Build/Pull Microservice images</a></li>
<li class="toctree-l3"><a class="reference internal" href="#build-mega-service-images">Build Mega Service images</a></li>
<li class="toctree-l3"><a class="reference internal" href="#build-other-service-images">Build Other Service images</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sanity-check">Sanity Check</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#use-case-setup">Use Case Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dataprep">Dataprep</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vectordb">VectorDB</a></li>
<li class="toctree-l3"><a class="reference internal" href="#embedding-service">Embedding Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reranking-service">Reranking Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="#llm-service">LLM Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="#megaservice">Megaservice</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-the-use-case">Deploy the use case</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#validate-microservice">Validate microservice</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#interacting-with-chatqna-deployment">Interacting with ChatQnA deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dataprep-microservice-optional">Dataprep Microservice（Optional）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tei-embedding-service">TEI Embedding Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="#embedding-microservice">Embedding Microservice</a></li>
<li class="toctree-l3"><a class="reference internal" href="#retriever-microservice">Retriever Microservice</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tei-reranking-service">TEI Reranking Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reranking-microservice">Reranking Microservice</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vllm-and-tgi-service">vLLM and TGI Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="#llm-microservice">LLM Microservice</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">MegaService</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#check-docker-container-log">Check docker container log</a></li>
<li class="toctree-l2"><a class="reference internal" href="#launch-ui">Launch UI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#basic-ui">Basic UI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conversational-ui">Conversational UI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stop-the-services">Stop the services</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TestOp</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Single node on-prem deployment with vLLM or TGI on Xeon Scalable processors</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/test.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="single-node-on-prem-deployment-with-vllm-or-tgi-on-xeon-scalable-processors">
<h1>Single node on-prem deployment with vLLM or TGI on Xeon Scalable processors<a class="headerlink" href="#single-node-on-prem-deployment-with-vllm-or-tgi-on-xeon-scalable-processors" title="Link to this heading">¶</a></h1>
<p>This deployment section covers single-node on-prem deployment of the ChatQnA
example with OPEA comps to deploy using vLLM or TGI service. There are several
slice-n-dice ways to enable RAG with vectordb and LLM models, but here we will
be covering one option of doing it for convenience : we will be showcasing  how
to build an e2e chatQnA with Redis VectorDB and neural-chat-7b-v3-3 model,
deployed on IDC. For more information on how to setup IDC instance to proceed,
Please follow the instructions here (*** getting started section***). If you do
not have an IDC instance you can skip the step and make sure that all the
(<em><strong>system level validation</strong></em>) metrics are addressed such as docker versions.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">¶</a></h2>
<p>There are several ways to setup a ChatQnA use case. Here in this tutorial, we
will walk through how to enable the below list of microservices from OPEA
GenAIComps to deploy a single node vLLM or TGI megaservice solution.</p>
<ol class="arabic simple">
<li><p>Data Prep</p></li>
<li><p>Embedding</p></li>
<li><p>Retriever</p></li>
<li><p>Reranking</p></li>
<li><p>LLM with vLLM or TGI</p></li>
</ol>
<p>The solution is aimed to show how to use Redis vectordb for RAG and
neural-chat-7b-v3-3 model on Intel Xeon Scalable processors. We will go through
how to setup docker container to start a microservices and megaservice . The
solution will then utilize a sample Nike dataset which is in PDF format. Users
can then ask a question about Nike and get a chat-like response by default for
up to 1024 tokens. The solution is deployed with a UI. There are 2 modes you can
use:</p>
<ol class="arabic simple">
<li><p>Basic UI</p></li>
<li><p>Conversational UI</p></li>
</ol>
<p>Conversational UI is optional, but a feature supported in this example if you
are interested to use.</p>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">¶</a></h2>
<p>First step is to clone the GenAIExamples and GenAIComps. GenAIComps are
fundamental necessary components used to build examples you find in
GenAIExamples and deploy them as microservices.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">opea</span><span class="o">-</span><span class="n">project</span><span class="o">/</span><span class="n">GenAIComps</span><span class="o">.</span><span class="n">git</span>
<span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">opea</span><span class="o">-</span><span class="n">project</span><span class="o">/</span><span class="n">GenAIExamples</span><span class="o">.</span><span class="n">git</span>
</pre></div>
</div>
<p>Checkout the release tag</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">GenAIComps</span>
<span class="n">git</span> <span class="n">checkout</span> <span class="n">tags</span><span class="o">/</span><span class="n">v1</span><span class="mf">.0</span>
</pre></div>
</div>
<p>The examples utilize model weights from HuggingFace and langchain.</p>
<p>Setup your <a class="reference external" href="https://huggingface.co/">HuggingFace</a> account and generate
<a class="reference external" href="https://huggingface.co/docs/transformers.js/en/guides/private#step-1-generating-a-user-access-token">user access token</a>.</p>
<p>Setup the HuggingFace token</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">HUGGINGFACEHUB_API_TOKEN</span><span class="o">=</span><span class="s2">&quot;Your_Huggingface_API_Token&quot;</span>
</pre></div>
</div>
<p>The example requires you to set the <code class="docutils literal notranslate"><span class="pre">host_ip</span></code> to deploy the microservices on
endpoint enabled with ports. Set the host_ip env variable</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export host_ip=$(hostname -I | awk &#39;{print $1}&#39;)
</pre></div>
</div>
<p>Make sure to setup Proxies if you are behind a firewall</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export no_proxy=${your_no_proxy},$host_ip
export http_proxy=${your_http_proxy}
export https_proxy=${your_http_proxy}
</pre></div>
</div>
</section>
<section id="prepare-building-pulling-docker-images">
<h2>Prepare (Building / Pulling) Docker images<a class="headerlink" href="#prepare-building-pulling-docker-images" title="Link to this heading">¶</a></h2>
<p>This step will involve building/pulling ( maybe in future) relevant docker
images with step-by-step process along with sanity check in the end. For
ChatQnA, the following docker images will be needed: embedding, retriever,
rerank, LLM and dataprep. Additionally, you will need to build docker images for
ChatQnA megaservice, and UI (conversational React UI is optional). In total,
there are 8 required and an optional docker images.</p>
<p>The docker images needed to setup the example needs to be build local, however
the images will be pushed to docker hub soon by Intel.</p>
<section id="build-pull-microservice-images">
<h3>Build/Pull Microservice images<a class="headerlink" href="#build-pull-microservice-images" title="Link to this heading">¶</a></h3>
<p>From within the <code class="docutils literal notranslate"><span class="pre">GenAIComps</span></code> folder</p>
<section id="build-dataprep-image">
<h4>Build Dataprep Image<a class="headerlink" href="#build-dataprep-image" title="Link to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker build --no-cache -t opea/dataprep-redis:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy -f comps/dataprep/redis/langchain/Dockerfile .
</pre></div>
</div>
</section>
<section id="build-embedding-image">
<h4>Build Embedding Image<a class="headerlink" href="#build-embedding-image" title="Link to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker build --no-cache -t opea/embedding-tei:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy -f comps/embeddings/tei/langchain/Dockerfile .
</pre></div>
</div>
</section>
<section id="build-retriever-image">
<h4>Build Retriever Image<a class="headerlink" href="#build-retriever-image" title="Link to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> docker build --no-cache -t opea/retriever-redis:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy -f comps/retrievers/redis/langchain/Dockerfile .
</pre></div>
</div>
</section>
<section id="build-rerank-image">
<h4>Build Rerank Image<a class="headerlink" href="#build-rerank-image" title="Link to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker build --no-cache -t opea/reranking-tei:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy -f comps/reranks/tei/Dockerfile .
</pre></div>
</div>
</section>
<section id="build-llm-image">
<h4>Build LLM Image<a class="headerlink" href="#build-llm-image" title="Link to this heading">¶</a></h4>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-0">
vllm</label><div class="sd-tab-content docutils">
<p>We build the vllm docker image from source</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/vllm-project/vllm.git
cd ./vllm/
docker build --no-cache -t opea/vllm:latest --build-arg https_proxy=$https_proxy \
   --build-arg http_proxy=$http_proxy -f Dockerfile.cpu .
cd ..
</pre></div>
</div>
<p>Next, we’ll build the vllm microservice docker. This will set the entry point
needed for the vllm to suit the ChatQnA examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker build --no-cache -t opea/llm-vllm:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy \
  -f comps/llms/text-generation/vllm/langchain/Dockerfile.microservice .

</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-1">
TGI</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker build --no-cache -t opea/llm-tgi:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy -f comps/llms/text-generation/tgi/Dockerfile .
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="build-mega-service-images">
<h3>Build Mega Service images<a class="headerlink" href="#build-mega-service-images" title="Link to this heading">¶</a></h3>
<p>The Megaservice is a pipeline that channels data through different
microservices, each performing varied tasks. We define the different
microservices and the flow of data between them in the <code class="docutils literal notranslate"><span class="pre">chatqna.py</span></code> file, say in
this example the output of embedding microservice will be the input of retrieval
microservice which will in turn passes data to the reranking microservice and so
on. You can also add newer or remove some microservices and customize the
megaservice to suit the needs.</p>
<p>Build the megaservice image for this use case</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">..</span>
<span class="n">cd</span> <span class="n">GenAIExamples</span><span class="o">/</span><span class="n">ChatQnA</span>
<span class="n">git</span> <span class="n">checkout</span> <span class="n">tags</span><span class="o">/</span><span class="n">v1</span><span class="mf">.0</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker build --no-cache -t opea/chatqna:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy -f Dockerfile .
</pre></div>
</div>
</section>
<section id="build-other-service-images">
<h3>Build Other Service images<a class="headerlink" href="#build-other-service-images" title="Link to this heading">¶</a></h3>
<section id="build-the-ui-image">
<h4>Build the UI Image<a class="headerlink" href="#build-the-ui-image" title="Link to this heading">¶</a></h4>
<p>As mentioned, you can build 2 modes of UI</p>
<p><em>Basic UI</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cd GenAIExamples/ChatQnA/ui/
docker build --no-cache -t opea/chatqna-ui:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy -f ./docker/Dockerfile .
</pre></div>
</div>
<p><em>Conversation UI</em>
If you want a conversational experience with chatqna megaservice.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cd GenAIExamples/ChatQnA/ui/
docker build --no-cache -t opea/chatqna-conversation-ui:latest --build-arg https_proxy=$https_proxy \
  --build-arg http_proxy=$http_proxy -f ./docker/Dockerfile.react .
</pre></div>
</div>
</section>
</section>
<section id="sanity-check">
<h3>Sanity Check<a class="headerlink" href="#sanity-check" title="Link to this heading">¶</a></h3>
<p>Check if you have the below set of docker images, before moving on to the next step:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-2" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-2">
vllm</label><div class="sd-tab-content docutils">
<ul class="simple">
<li><p>opea/dataprep-redis:latest</p></li>
<li><p>opea/embedding-tei:latest</p></li>
<li><p>opea/retriever-redis:latest</p></li>
<li><p>opea/reranking-tei:latest</p></li>
<li><p>opea/vllm:latest</p></li>
<li><p>opea/chatqna:latest</p></li>
<li><p>opea/chatqna-ui:latest</p></li>
<li><p>opea/llm-vllm:latest</p></li>
</ul>
</div>
<input id="sd-tab-item-3" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-3">
TGI</label><div class="sd-tab-content docutils">
<ul class="simple">
<li><p>opea/dataprep-redis:latest</p></li>
<li><p>opea/embedding-tei:latest</p></li>
<li><p>opea/retriever-redis:latest</p></li>
<li><p>opea/reranking-tei:latest</p></li>
<li><p>opea/chatqna:latest</p></li>
<li><p>opea/chatqna-ui:latest</p></li>
<li><p>opea/llm-tgi:latest</p></li>
</ul>
</div>
</div>
</section>
</section>
<section id="use-case-setup">
<h2>Use Case Setup<a class="headerlink" href="#use-case-setup" title="Link to this heading">¶</a></h2>
<p>As mentioned the use case will use the following combination of the GenAIComps
with the tools</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-4" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-4">
vllm</label><div class="sd-tab-content docutils">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>use case components</p></th>
<th class="head"><p>Tools</p></th>
<th class="head"><p>Model</p></th>
<th class="head"><p>Service Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data Prep</p></td>
<td><p>LangChain</p></td>
<td><p>NA</p></td>
<td><p>OPEA Microservice</p></td>
</tr>
<tr class="row-odd"><td><p>VectorDB</p></td>
<td><p>Redis</p></td>
<td><p>NA</p></td>
<td><p>Open source service</p></td>
</tr>
<tr class="row-even"><td><p>Embedding</p></td>
<td><p>TEI</p></td>
<td><p>BAAI/bge-base-en-v1.5</p></td>
<td><p>OPEA Microservice</p></td>
</tr>
<tr class="row-odd"><td><p>Reranking</p></td>
<td><p>TEI</p></td>
<td><p>BAAI/bge-reranker-base</p></td>
<td><p>OPEA Microservice</p></td>
</tr>
<tr class="row-even"><td><p>LLM</p></td>
<td><p>vLLM</p></td>
<td><p>Intel/neural-chat-7b-v3-3</p></td>
<td><p>OPEA Microservice</p></td>
</tr>
<tr class="row-odd"><td><p>UI</p></td>
<td><p></p></td>
<td><p>NA</p></td>
<td><p>Gateway Service</p></td>
</tr>
</tbody>
</table>
<p>Tools and models mentioned in the table are configurable either through the
environment variable or <code class="docutils literal notranslate"><span class="pre">compose_vllm.yaml</span></code> file.</p>
</div>
<input id="sd-tab-item-5" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-5">
TGI</label><div class="sd-tab-content docutils">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>use case components</p></th>
<th class="head"><p>Tools</p></th>
<th class="head"><p>Model</p></th>
<th class="head"><p>Service Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data Prep</p></td>
<td><p>LangChain</p></td>
<td><p>NA</p></td>
<td><p>OPEA Microservice</p></td>
</tr>
<tr class="row-odd"><td><p>VectorDB</p></td>
<td><p>Redis</p></td>
<td><p>NA</p></td>
<td><p>Open source service</p></td>
</tr>
<tr class="row-even"><td><p>Embedding</p></td>
<td><p>TEI</p></td>
<td><p>BAAI/bge-base-en-v1.5</p></td>
<td><p>OPEA Microservice</p></td>
</tr>
<tr class="row-odd"><td><p>Reranking</p></td>
<td><p>TEI</p></td>
<td><p>BAAI/bge-reranker-base</p></td>
<td><p>OPEA Microservice</p></td>
</tr>
<tr class="row-even"><td><p>LLM</p></td>
<td><p>TGI</p></td>
<td><p>Intel/neural-chat-7b-v3-3</p></td>
<td><p>OPEA Microservice</p></td>
</tr>
<tr class="row-odd"><td><p>UI</p></td>
<td><p></p></td>
<td><p>NA</p></td>
<td><p>Gateway Service</p></td>
</tr>
</tbody>
</table>
<p>Tools and models mentioned in the table are configurable either through the
environment variable or <code class="docutils literal notranslate"><span class="pre">compose.yaml</span></code> file.</p>
</div>
</div>
<p>Set the necessary environment variables to setup the use case case</p>
<blockquote>
<div><p>Note: Replace <code class="docutils literal notranslate"><span class="pre">host_ip</span></code> with your external IP address. Do <strong>NOT</strong> use localhost
for the below set of environment variables</p>
</div></blockquote>
<section id="dataprep">
<h3>Dataprep<a class="headerlink" href="#dataprep" title="Link to this heading">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>export DATAPREP_SERVICE_ENDPOINT=&quot;http://${host_ip}:6007/v1/dataprep&quot;
export DATAPREP_GET_FILE_ENDPOINT=&quot;http://${host_ip}:6007/v1/dataprep/get_file&quot;
export DATAPREP_DELETE_FILE_ENDPOINT=&quot;http://${host_ip}:6007/v1/dataprep/delete_file&quot;
</pre></div>
</div>
</section>
<section id="vectordb">
<h3>VectorDB<a class="headerlink" href="#vectordb" title="Link to this heading">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>export REDIS_URL=&quot;redis://${host_ip}:6379&quot;
export INDEX_NAME=&quot;rag-redis&quot;
</pre></div>
</div>
</section>
<section id="embedding-service">
<h3>Embedding Service<a class="headerlink" href="#embedding-service" title="Link to this heading">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>export EMBEDDING_MODEL_ID=&quot;BAAI/bge-base-en-v1.5&quot;
export EMBEDDING_SERVICE_HOST_IP=${host_ip}
export RETRIEVER_SERVICE_HOST_IP=${host_ip}
export TEI_EMBEDDING_ENDPOINT=&quot;http://${host_ip}:6006&quot;
</pre></div>
</div>
</section>
<section id="reranking-service">
<h3>Reranking Service<a class="headerlink" href="#reranking-service" title="Link to this heading">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>export RERANK_MODEL_ID=&quot;BAAI/bge-reranker-base&quot;
export TEI_RERANKING_ENDPOINT=&quot;http://${host_ip}:8808&quot;
export RERANK_SERVICE_HOST_IP=${host_ip}
</pre></div>
</div>
</section>
<section id="llm-service">
<h3>LLM Service<a class="headerlink" href="#llm-service" title="Link to this heading">¶</a></h3>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-6" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-6">
vllm</label><div class="sd-tab-content docutils">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>export LLM_MODEL_ID=&quot;Intel/neural-chat-7b-v3-3&quot;
export LLM_SERVICE_HOST_IP=${host_ip}
export LLM_SERVICE_PORT=9000
export vLLM_LLM_ENDPOINT=&quot;http://${host_ip}:9009&quot;
</pre></div>
</div>
</div>
<input id="sd-tab-item-7" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-7">
TGI</label><div class="sd-tab-content docutils">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>export LLM_MODEL_ID=&quot;Intel/neural-chat-7b-v3-3&quot;
export LLM_SERVICE_HOST_IP=${host_ip}
export LLM_SERVICE_PORT=9000
export TGI_LLM_ENDPOINT=&quot;http://${host_ip}:9009&quot;
</pre></div>
</div>
</div>
</div>
</section>
<section id="megaservice">
<h3>Megaservice<a class="headerlink" href="#megaservice" title="Link to this heading">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>export MEGA_SERVICE_HOST_IP=${host_ip}
export BACKEND_SERVICE_ENDPOINT=&quot;http://${host_ip}:8888/v1/chatqna&quot;
</pre></div>
</div>
</section>
</section>
<section id="deploy-the-use-case">
<h2>Deploy the use case<a class="headerlink" href="#deploy-the-use-case" title="Link to this heading">¶</a></h2>
<p>In this tutorial, we will be deploying via docker compose with the provided
YAML file.  The docker compose instructions should be starting all the
above mentioned services as containers.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-8" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-8">
vllm</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">GenAIExamples</span><span class="o">/</span><span class="n">ChatQnA</span><span class="o">/</span><span class="n">docker_compose</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">cpu</span><span class="o">/</span><span class="n">xeon</span>
<span class="n">docker</span> <span class="n">compose</span> <span class="o">-</span><span class="n">f</span> <span class="n">compose_vllm</span><span class="o">.</span><span class="n">yaml</span> <span class="n">up</span> <span class="o">-</span><span class="n">d</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-9" name="sd-tab-set-4" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-9">
TGI</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">GenAIExamples</span><span class="o">/</span><span class="n">ChatQnA</span><span class="o">/</span><span class="n">docker_compose</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">cpu</span><span class="o">/</span><span class="n">xeon</span>
<span class="n">docker</span> <span class="n">compose</span> <span class="o">-</span><span class="n">f</span> <span class="n">compose</span><span class="o">.</span><span class="n">yaml</span> <span class="n">up</span> <span class="o">-</span><span class="n">d</span>
</pre></div>
</div>
</div>
</div>
<section id="validate-microservice">
<h3>Validate microservice<a class="headerlink" href="#validate-microservice" title="Link to this heading">¶</a></h3>
<section id="check-env-variables">
<h4>Check Env Variables<a class="headerlink" href="#check-env-variables" title="Link to this heading">¶</a></h4>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-10" name="sd-tab-set-5" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-10">
vllm</label><div class="sd-tab-content docutils">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Check the start up log by `docker compose -f ./compose_vllm.yaml logs`.
</pre></div>
</div>
<p>The warning messages print out the variables if they are <strong>NOT</strong> set.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>ubuntu@xeon-vm:~/GenAIExamples/ChatQnA/docker_compose/intel/cpu/xeon$ docker compose -f ./compose_vllm.yaml up -d
WARN[0000] The &quot;LANGCHAIN_API_KEY&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_TRACING_V2&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_API_KEY&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_TRACING_V2&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_API_KEY&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_TRACING_V2&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_API_KEY&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_TRACING_V2&quot; variable is not set. Defaulting to a blank string.
WARN[0000] /home/ubuntu/GenAIExamples/ChatQnA/docker_compose/intel/cpu/xeon/compose_vllm.yaml: `version` is obsolete
</pre></div>
</div>
</div>
<input id="sd-tab-item-11" name="sd-tab-set-5" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-11">
TGI</label><div class="sd-tab-content docutils">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Check the start up log by `docker compose -f ./compose.yaml logs`.
</pre></div>
</div>
<p>The warning messages print out the variables if they are <strong>NOT</strong> set.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>ubuntu@xeon-vm:~/GenAIExamples/ChatQnA/docker_compose/intel/cpu/xeon$ docker compose -f ./compose.yaml up -d
WARN[0000] The &quot;LANGCHAIN_API_KEY&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_TRACING_V2&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_API_KEY&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_TRACING_V2&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_API_KEY&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_TRACING_V2&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_API_KEY&quot; variable is not set. Defaulting to a blank string.
WARN[0000] The &quot;LANGCHAIN_TRACING_V2&quot; variable is not set. Defaulting to a blank string.
WARN[0000] /home/ubuntu/GenAIExamples/ChatQnA/docker_compose/intel/cpu/xeon/compose.yaml: `version` is obsolete
</pre></div>
</div>
</div>
</div>
</section>
<section id="check-the-container-status">
<h4>Check the container status<a class="headerlink" href="#check-the-container-status" title="Link to this heading">¶</a></h4>
<p>Check if all the containers  launched via docker compose has started</p>
<p>For example, the ChatQnA example starts 11 docker (services), check these docker
containers are all running, i.e, all the containers  <code class="docutils literal notranslate"><span class="pre">STATUS</span></code>  are  <code class="docutils literal notranslate"><span class="pre">Up</span></code>
To do a quick sanity check, try <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">ps</span> <span class="pre">-a</span></code> to see if all the containers are running</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-12" name="sd-tab-set-6" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-12">
vllm</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CONTAINER</span> <span class="n">ID</span>   <span class="n">IMAGE</span>                                                   <span class="n">COMMAND</span>                  <span class="n">CREATED</span>        <span class="n">STATUS</span>        <span class="n">PORTS</span>                                                                                  <span class="n">NAMES</span>
<span class="mi">3</span><span class="n">b5fa9a722da</span>   <span class="n">opea</span><span class="o">/</span><span class="n">chatqna</span><span class="o">-</span><span class="n">ui</span><span class="p">:</span><span class="n">latest</span>                                  <span class="s2">&quot;docker-entrypoint.s…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">5173</span><span class="o">-&gt;</span><span class="mi">5173</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">5173</span><span class="o">-&gt;</span><span class="mi">5173</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">chatqna</span><span class="o">-</span><span class="n">xeon</span><span class="o">-</span><span class="n">ui</span><span class="o">-</span><span class="n">server</span>
<span class="n">d3b37f3d1faa</span>   <span class="n">opea</span><span class="o">/</span><span class="n">chatqna</span><span class="p">:</span><span class="n">latest</span>                                     <span class="s2">&quot;python chatqna.py&quot;</span>      <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">8888</span><span class="o">-&gt;</span><span class="mi">8888</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8888</span><span class="o">-&gt;</span><span class="mi">8888</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">chatqna</span><span class="o">-</span><span class="n">xeon</span><span class="o">-</span><span class="n">backend</span><span class="o">-</span><span class="n">server</span>
<span class="n">b3e1388fa2ca</span>   <span class="n">opea</span><span class="o">/</span><span class="n">reranking</span><span class="o">-</span><span class="n">tei</span><span class="p">:</span><span class="n">latest</span>                               <span class="s2">&quot;python reranking_te…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">8000</span><span class="o">-&gt;</span><span class="mi">8000</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8000</span><span class="o">-&gt;</span><span class="mi">8000</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">reranking</span><span class="o">-</span><span class="n">tei</span><span class="o">-</span><span class="n">xeon</span><span class="o">-</span><span class="n">server</span>
<span class="mi">24</span><span class="n">a240f8ad1c</span>   <span class="n">opea</span><span class="o">/</span><span class="n">retriever</span><span class="o">-</span><span class="n">redis</span><span class="p">:</span><span class="n">latest</span>                             <span class="s2">&quot;python retriever_re…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">7000</span><span class="o">-&gt;</span><span class="mi">7000</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">7000</span><span class="o">-&gt;</span><span class="mi">7000</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">retriever</span><span class="o">-</span><span class="n">redis</span><span class="o">-</span><span class="n">server</span>
<span class="mi">9</span><span class="n">c0d2a2553e8</span>   <span class="n">opea</span><span class="o">/</span><span class="n">embedding</span><span class="o">-</span><span class="n">tei</span><span class="p">:</span><span class="n">latest</span>                               <span class="s2">&quot;python embedding_te…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">6000</span><span class="o">-&gt;</span><span class="mi">6000</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">6000</span><span class="o">-&gt;</span><span class="mi">6000</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">embedding</span><span class="o">-</span><span class="n">tei</span><span class="o">-</span><span class="n">server</span>
<span class="mi">24</span><span class="n">cae0db1a70</span>   <span class="n">opea</span><span class="o">/</span><span class="n">llm</span><span class="o">-</span><span class="n">vllm</span><span class="p">:</span><span class="n">latest</span>                                    <span class="s2">&quot;bash entrypoint.sh&quot;</span>     <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">9000</span><span class="o">-&gt;</span><span class="mi">9000</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">9000</span><span class="o">-&gt;</span><span class="mi">9000</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">llm</span><span class="o">-</span><span class="n">vllm</span><span class="o">-</span><span class="n">server</span>
<span class="n">ea3986c3cf82</span>   <span class="n">opea</span><span class="o">/</span><span class="n">dataprep</span><span class="o">-</span><span class="n">redis</span><span class="p">:</span><span class="n">latest</span>                              <span class="s2">&quot;python prepare_doc_…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">6007</span><span class="o">-&gt;</span><span class="mi">6007</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">6007</span><span class="o">-&gt;</span><span class="mi">6007</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">dataprep</span><span class="o">-</span><span class="n">redis</span><span class="o">-</span><span class="n">server</span>
<span class="n">e10dd14497a8</span>   <span class="n">redis</span><span class="o">/</span><span class="n">redis</span><span class="o">-</span><span class="n">stack</span><span class="p">:</span><span class="mf">7.2.0</span><span class="o">-</span><span class="n">v9</span>                              <span class="s2">&quot;/entrypoint.sh&quot;</span>         <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">6379</span><span class="o">-&gt;</span><span class="mi">6379</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">6379</span><span class="o">-&gt;</span><span class="mi">6379</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">8001</span><span class="o">-&gt;</span><span class="mi">8001</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8001</span><span class="o">-&gt;</span><span class="mi">8001</span><span class="o">/</span><span class="n">tcp</span>   <span class="n">redis</span><span class="o">-</span><span class="n">vector</span><span class="o">-</span><span class="n">db</span>
<span class="n">b98fa07a4f5c</span>   <span class="n">opea</span><span class="o">/</span><span class="n">vllm</span><span class="p">:</span><span class="n">latest</span>                                        <span class="s2">&quot;python3 -m vllm.ent…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">9009</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">9009</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span>                                                  <span class="n">vllm</span><span class="o">-</span><span class="n">service</span>
<span class="mi">79276</span><span class="n">cf45a47</span>   <span class="n">ghcr</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">text</span><span class="o">-</span><span class="n">embeddings</span><span class="o">-</span><span class="n">inference</span><span class="p">:</span><span class="n">cpu</span><span class="o">-</span><span class="mf">1.2</span>   <span class="s2">&quot;text-embeddings-rou…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">6006</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">6006</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span>                                                  <span class="n">tei</span><span class="o">-</span><span class="n">embedding</span><span class="o">-</span><span class="n">server</span>
<span class="mf">4943e5</span><span class="n">f6cd80</span>   <span class="n">ghcr</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">text</span><span class="o">-</span><span class="n">embeddings</span><span class="o">-</span><span class="n">inference</span><span class="p">:</span><span class="n">cpu</span><span class="o">-</span><span class="mf">1.2</span>   <span class="s2">&quot;text-embeddings-rou…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">8808</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8808</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span>                                                  <span class="n">tei</span><span class="o">-</span><span class="n">reranking</span><span class="o">-</span><span class="n">server</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-13" name="sd-tab-set-6" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-13">
TGI</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CONTAINER</span> <span class="n">ID</span>   <span class="n">IMAGE</span>                                                   <span class="n">COMMAND</span>                  <span class="n">CREATED</span>        <span class="n">STATUS</span>        <span class="n">PORTS</span>                                                                                  <span class="n">NAMES</span>
<span class="mi">3</span><span class="n">b5fa9a722da</span>   <span class="n">opea</span><span class="o">/</span><span class="n">chatqna</span><span class="o">-</span><span class="n">ui</span><span class="p">:</span><span class="n">latest</span>                                  <span class="s2">&quot;docker-entrypoint.s…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">5173</span><span class="o">-&gt;</span><span class="mi">5173</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">5173</span><span class="o">-&gt;</span><span class="mi">5173</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">chatqna</span><span class="o">-</span><span class="n">xeon</span><span class="o">-</span><span class="n">ui</span><span class="o">-</span><span class="n">server</span>
<span class="n">d3b37f3d1faa</span>   <span class="n">opea</span><span class="o">/</span><span class="n">chatqna</span><span class="p">:</span><span class="n">latest</span>                                     <span class="s2">&quot;python chatqna.py&quot;</span>      <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">8888</span><span class="o">-&gt;</span><span class="mi">8888</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8888</span><span class="o">-&gt;</span><span class="mi">8888</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">chatqna</span><span class="o">-</span><span class="n">xeon</span><span class="o">-</span><span class="n">backend</span><span class="o">-</span><span class="n">server</span>
<span class="n">b3e1388fa2ca</span>   <span class="n">opea</span><span class="o">/</span><span class="n">reranking</span><span class="o">-</span><span class="n">tei</span><span class="p">:</span><span class="n">latest</span>                               <span class="s2">&quot;python reranking_te…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">8000</span><span class="o">-&gt;</span><span class="mi">8000</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8000</span><span class="o">-&gt;</span><span class="mi">8000</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">reranking</span><span class="o">-</span><span class="n">tei</span><span class="o">-</span><span class="n">xeon</span><span class="o">-</span><span class="n">server</span>
<span class="mi">24</span><span class="n">a240f8ad1c</span>   <span class="n">opea</span><span class="o">/</span><span class="n">retriever</span><span class="o">-</span><span class="n">redis</span><span class="p">:</span><span class="n">latest</span>                             <span class="s2">&quot;python retriever_re…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">7000</span><span class="o">-&gt;</span><span class="mi">7000</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">7000</span><span class="o">-&gt;</span><span class="mi">7000</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">retriever</span><span class="o">-</span><span class="n">redis</span><span class="o">-</span><span class="n">server</span>
<span class="mi">9</span><span class="n">c0d2a2553e8</span>   <span class="n">opea</span><span class="o">/</span><span class="n">embedding</span><span class="o">-</span><span class="n">tei</span><span class="p">:</span><span class="n">latest</span>                               <span class="s2">&quot;python embedding_te…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">6000</span><span class="o">-&gt;</span><span class="mi">6000</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">6000</span><span class="o">-&gt;</span><span class="mi">6000</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">embedding</span><span class="o">-</span><span class="n">tei</span><span class="o">-</span><span class="n">server</span>
<span class="mi">24</span><span class="n">cae0db1a70</span>   <span class="n">opea</span><span class="o">/</span><span class="n">llm</span><span class="o">-</span><span class="n">tgi</span><span class="p">:</span><span class="n">latest</span>                                    <span class="s2">&quot;bash entrypoint.sh&quot;</span>     <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">9000</span><span class="o">-&gt;</span><span class="mi">9000</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">9000</span><span class="o">-&gt;</span><span class="mi">9000</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">llm</span><span class="o">-</span><span class="n">tgi</span><span class="o">-</span><span class="n">server</span>
<span class="n">ea3986c3cf82</span>   <span class="n">opea</span><span class="o">/</span><span class="n">dataprep</span><span class="o">-</span><span class="n">redis</span><span class="p">:</span><span class="n">latest</span>                              <span class="s2">&quot;python prepare_doc_…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">6007</span><span class="o">-&gt;</span><span class="mi">6007</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">6007</span><span class="o">-&gt;</span><span class="mi">6007</span><span class="o">/</span><span class="n">tcp</span>                                              <span class="n">dataprep</span><span class="o">-</span><span class="n">redis</span><span class="o">-</span><span class="n">server</span>
<span class="n">e10dd14497a8</span>   <span class="n">redis</span><span class="o">/</span><span class="n">redis</span><span class="o">-</span><span class="n">stack</span><span class="p">:</span><span class="mf">7.2.0</span><span class="o">-</span><span class="n">v9</span>                              <span class="s2">&quot;/entrypoint.sh&quot;</span>         <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">6379</span><span class="o">-&gt;</span><span class="mi">6379</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">6379</span><span class="o">-&gt;</span><span class="mi">6379</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">8001</span><span class="o">-&gt;</span><span class="mi">8001</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8001</span><span class="o">-&gt;</span><span class="mi">8001</span><span class="o">/</span><span class="n">tcp</span>   <span class="n">redis</span><span class="o">-</span><span class="n">vector</span><span class="o">-</span><span class="n">db</span>
<span class="mi">79276</span><span class="n">cf45a47</span>   <span class="n">ghcr</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">text</span><span class="o">-</span><span class="n">embeddings</span><span class="o">-</span><span class="n">inference</span><span class="p">:</span><span class="n">cpu</span><span class="o">-</span><span class="mf">1.2</span>   <span class="s2">&quot;text-embeddings-rou…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">6006</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">6006</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span>                                                  <span class="n">tei</span><span class="o">-</span><span class="n">embedding</span><span class="o">-</span><span class="n">server</span>
<span class="mf">4943e5</span><span class="n">f6cd80</span>   <span class="n">ghcr</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">text</span><span class="o">-</span><span class="n">embeddings</span><span class="o">-</span><span class="n">inference</span><span class="p">:</span><span class="n">cpu</span><span class="o">-</span><span class="mf">1.2</span>   <span class="s2">&quot;text-embeddings-rou…&quot;</span>   <span class="mi">32</span> <span class="n">hours</span> <span class="n">ago</span>   <span class="n">Up</span> <span class="mi">2</span> <span class="n">hours</span>   <span class="mf">0.0.0.0</span><span class="p">:</span><span class="mi">8808</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span><span class="p">,</span> <span class="p">:::</span><span class="mi">8808</span><span class="o">-&gt;</span><span class="mi">80</span><span class="o">/</span><span class="n">tcp</span>                                                  <span class="n">tei</span><span class="o">-</span><span class="n">reranking</span><span class="o">-</span><span class="n">server</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="interacting-with-chatqna-deployment">
<h2>Interacting with ChatQnA deployment<a class="headerlink" href="#interacting-with-chatqna-deployment" title="Link to this heading">¶</a></h2>
<p>This section will walk you through what are the different ways to interact with
the microservices deployed</p>
<section id="dataprep-microservice-optional">
<h3>Dataprep Microservice（Optional）<a class="headerlink" href="#dataprep-microservice-optional" title="Link to this heading">¶</a></h3>
<p>If you want to add/update the default knowledge base, you can use the following
commands. The dataprep microservice extracts the texts from variety of data
sources, chunks the data, embeds each chunk using embedding microservice and
store the embedded vectors in the redis vector database.</p>
<p>Local File <code class="docutils literal notranslate"><span class="pre">nke-10k-2023.pdf</span></code> Upload:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="s2">&quot;http://$</span><span class="si">{host_ip}</span><span class="s2">:6007/v1/dataprep&quot;</span> \
     <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: multipart/form-data&quot;</span> \
     <span class="o">-</span><span class="n">F</span> <span class="s2">&quot;files=@./nke-10k-2023.pdf&quot;</span>
</pre></div>
</div>
<p>This command updates a knowledge base by uploading a local file for processing.
Update the file path according to your environment.</p>
<p>Add Knowledge Base via HTTP Links:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="s2">&quot;http://$</span><span class="si">{host_ip}</span><span class="s2">:6007/v1/dataprep&quot;</span> \
     <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: multipart/form-data&quot;</span> \
     <span class="o">-</span><span class="n">F</span> <span class="s1">&#39;link_list=[&quot;https://opea.dev&quot;]&#39;</span>
</pre></div>
</div>
<p>This command updates a knowledge base by submitting a list of HTTP links for processing.</p>
<p>Also, you are able to get the file list that you uploaded:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="s2">&quot;http://$</span><span class="si">{host_ip}</span><span class="s2">:6007/v1/dataprep/get_file&quot;</span> \
     <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span>

</pre></div>
</div>
<p>To delete the file/link you uploaded you can use the following commands:</p>
<section id="delete-link">
<h4>Delete link<a class="headerlink" href="#delete-link" title="Link to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The dataprep service will add a .txt postfix for link file</span>

<span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="s2">&quot;http://$</span><span class="si">{host_ip}</span><span class="s2">:6007/v1/dataprep/delete_file&quot;</span> \
     <span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{&quot;file_path&quot;: &quot;https://opea.dev.txt&quot;}&#39;</span> \
     <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span>
</pre></div>
</div>
</section>
<section id="delete-file">
<h4>Delete file<a class="headerlink" href="#delete-file" title="Link to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="s2">&quot;http://$</span><span class="si">{host_ip}</span><span class="s2">:6007/v1/dataprep/delete_file&quot;</span> \
     <span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{&quot;file_path&quot;: &quot;nke-10k-2023.pdf&quot;}&#39;</span> \
     <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span>
</pre></div>
</div>
</section>
<section id="delete-all-uploaded-files-and-links">
<h4>Delete all uploaded files and links<a class="headerlink" href="#delete-all-uploaded-files-and-links" title="Link to this heading">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="s2">&quot;http://$</span><span class="si">{host_ip}</span><span class="s2">:6007/v1/dataprep/delete_file&quot;</span> \
     <span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{&quot;file_path&quot;: &quot;all&quot;}&#39;</span> \
     <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="tei-embedding-service">
<h3>TEI Embedding Service<a class="headerlink" href="#tei-embedding-service" title="Link to this heading">¶</a></h3>
<p>The TEI embedding service takes in a string as input, embeds the string into a
vector of a specific length determined by the embedding model and returns this
embedded vector.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl ${host_ip}:6006/embed \
    -X POST \
    -d &#39;{&quot;inputs&quot;:&quot;What is Deep Learning?&quot;}&#39; \
    -H &#39;Content-Type: application/json&#39;
</pre></div>
</div>
<p>In this example the embedding model used is “BAAI/bge-base-en-v1.5”, which has a
vector size of 768. So the output of the curl command is a embedded vector of
length 768.</p>
</section>
<section id="embedding-microservice">
<h3>Embedding Microservice<a class="headerlink" href="#embedding-microservice" title="Link to this heading">¶</a></h3>
<p>The embedding microservice depends on the TEI embedding service. In terms of
input parameters, it takes in a string, embeds it into a vector using the TEI
embedding service and pads other default parameters that are required for the
retrieval microservice and returns it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl http://${host_ip}:6000/v1/embeddings\
  -X POST \
  -d &#39;{&quot;text&quot;:&quot;hello&quot;}&#39; \
  -H &#39;Content-Type: application/json&#39;
</pre></div>
</div>
</section>
<section id="retriever-microservice">
<h3>Retriever Microservice<a class="headerlink" href="#retriever-microservice" title="Link to this heading">¶</a></h3>
<p>To consume the retriever microservice, you need to generate a mock embedding
vector by Python script. The length of embedding vector is determined by the
embedding model. Here we use the
model EMBEDDING_MODEL_ID=”BAAI/bge-base-en-v1.5”, which vector size is 768.</p>
<p>Check the vector dimension of your embedding model and set
<code class="docutils literal notranslate"><span class="pre">your_embedding</span></code> dimension equal to it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export your_embedding=$(python3 -c &quot;import random; embedding = [random.uniform(-1, 1) for _ in range(768)]; print(embedding)&quot;)

curl http://${host_ip}:7000/v1/retrieval \
  -X POST \
  -d &quot;{\&quot;text\&quot;:\&quot;test\&quot;,\&quot;embedding\&quot;:${your_embedding}}&quot; \
  -H &#39;Content-Type: application/json&#39;

</pre></div>
</div>
<p>The output of the retriever microservice comprises of the a unique id for the
request, initial query or the input to the retrieval microservice, a list of top
<code class="docutils literal notranslate"><span class="pre">n</span></code> retrieved documents relevant to the input query, and top_n where n refers to
the number of documents to be returned.</p>
<p>The output is retrieved text that relevant to the input data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span><span class="s2">&quot;27210945c7c6c054fa7355bdd4cde818&quot;</span><span class="p">,</span><span class="s2">&quot;retrieved_docs&quot;</span><span class="p">:[{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span><span class="s2">&quot;0c1dd04b31ab87a5468d65f98e33a9f6&quot;</span><span class="p">,</span><span class="s2">&quot;text&quot;</span><span class="p">:</span><span class="s2">&quot;Company: Nike. financial instruments are subject to master netting arrangements that allow for the offset of assets and liabilities in the event of default or early termination of the contract.</span><span class="se">\n</span><span class="s2">Any amounts of cash collateral received related to these instruments associated with the Company&#39;s credit-related contingent features are recorded in Cash and</span><span class="se">\n</span><span class="s2">equivalents and Accrued liabilities, the latter of which would further offset against the Company&#39;s derivative asset balance. Any amounts of cash collateral posted related</span><span class="se">\n</span><span class="s2">to these instruments associated with the Company&#39;s credit-related contingent features are recorded in Prepaid expenses and other current assets, which would further</span><span class="se">\n</span><span class="s2">offset against the Company&#39;s derivative liability balance. Cash collateral received or posted related to the Company&#39;s credit-related contingent features is presented in the</span><span class="se">\n</span><span class="s2">Cash provided by operations component of the Consolidated Statements of Cash Flows. The Company does not recognize amounts of non-cash collateral received, such</span><span class="se">\n</span><span class="s2">as securities, on the Consolidated Balance Sheets. For further information related to credit risk, refer to Note 12 — Risk Management and Derivatives.</span><span class="se">\n</span><span class="s2">2023 FORM 10-K 68Table of Contents</span><span class="se">\n</span><span class="s2">The following tables present information about the Company&#39;s derivative assets and liabilities measured at fair value on a recurring basis and indicate the level in the fair</span><span class="se">\n</span><span class="s2">value hierarchy in which the Company classifies the fair value measurement:</span><span class="se">\n</span><span class="s2">MAY 31, 2023</span><span class="se">\n</span><span class="s2">DERIVATIVE ASSETS</span><span class="se">\n</span><span class="s2">DERIVATIVE LIABILITIES&quot;</span><span class="p">},{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span><span class="s2">&quot;1d742199fb1a86aa8c3f7bcd580d94af&quot;</span><span class="p">,</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="o">...</span> <span class="p">}</span>

</pre></div>
</div>
</section>
<section id="tei-reranking-service">
<h3>TEI Reranking Service<a class="headerlink" href="#tei-reranking-service" title="Link to this heading">¶</a></h3>
<p>The TEI Reranking Service reranks the documents returned by the retrieval
service. It consumes the query and list of documents and returns the document
index based on decreasing order of the similarity score. The document
corresponding to the returned index with the highest score is the most relevant
document for the input query.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl http://${host_ip}:8808/rerank \
    -X POST \
    -d &#39;{&quot;query&quot;:&quot;What is Deep Learning?&quot;, &quot;texts&quot;: [&quot;Deep Learning is not...&quot;, &quot;Deep learning is...&quot;]}&#39; \
    -H &#39;Content-Type: application/json&#39;
</pre></div>
</div>
<p>Output is:  <code class="docutils literal notranslate"><span class="pre">[{&quot;index&quot;:1,&quot;score&quot;:0.9988041},{&quot;index&quot;:0,&quot;score&quot;:0.022948774}]</span></code></p>
</section>
<section id="reranking-microservice">
<h3>Reranking Microservice<a class="headerlink" href="#reranking-microservice" title="Link to this heading">¶</a></h3>
<p>The reranking microservice consumes the TEI Reranking service and pads the
response with default parameters required for the llm microservice.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl http://${host_ip}:8000/v1/reranking\
  -X POST \
  -d &#39;{&quot;initial_query&quot;:&quot;What is Deep Learning?&quot;, &quot;retrieved_docs&quot;: \
     [{&quot;text&quot;:&quot;Deep Learning is not...&quot;}, {&quot;text&quot;:&quot;Deep learning is...&quot;}]}&#39; \
  -H &#39;Content-Type: application/json&#39;
</pre></div>
</div>
<p>The input to the microservice is the <code class="docutils literal notranslate"><span class="pre">initial_query</span></code> and a list of retrieved
documents and it outputs the most relevant document to the initial query along
with other default parameter such as temperature, <code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code>,
<code class="docutils literal notranslate"><span class="pre">chat_template</span></code> and so on. We can also get top n documents by setting <code class="docutils literal notranslate"><span class="pre">top_n</span></code> as one
of the input parameters. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl http://${host_ip}:8000/v1/reranking\
  -X POST \
  -d &#39;{&quot;initial_query&quot;:&quot;What is Deep Learning?&quot; ,&quot;top_n&quot;:2, &quot;retrieved_docs&quot;: \
     [{&quot;text&quot;:&quot;Deep Learning is not...&quot;}, {&quot;text&quot;:&quot;Deep learning is...&quot;}]}&#39; \
  -H &#39;Content-Type: application/json&#39;
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span><span class="s2">&quot;e1eb0e44f56059fc01aa0334b1dac313&quot;</span><span class="p">,</span><span class="s2">&quot;query&quot;</span><span class="p">:</span><span class="s2">&quot;Human: Answer the question based only on the following context:</span><span class="se">\n</span><span class="s2">    Deep learning is...</span><span class="se">\n</span><span class="s2">    Question: What is Deep Learning?&quot;</span><span class="p">,</span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span><span class="mi">1024</span><span class="p">,</span><span class="s2">&quot;top_k&quot;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span><span class="s2">&quot;top_p&quot;</span><span class="p">:</span><span class="mf">0.95</span><span class="p">,</span><span class="s2">&quot;typical_p&quot;</span><span class="p">:</span><span class="mf">0.95</span><span class="p">,</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span><span class="mf">0.01</span><span class="p">,</span><span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span><span class="mf">1.03</span><span class="p">,</span><span class="s2">&quot;streaming&quot;</span><span class="p">:</span><span class="n">true</span><span class="p">}</span>

</pre></div>
</div>
<p>You may notice reranking microservice are with state (‘ID’ and other meta data),
while reranking service are not.</p>
</section>
<section id="vllm-and-tgi-service">
<h3>vLLM and TGI Service<a class="headerlink" href="#vllm-and-tgi-service" title="Link to this heading">¶</a></h3>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-14" name="sd-tab-set-7" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-14">
vllm</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl http://${host_ip}:9009/v1/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;model&quot;: &quot;Intel/neural-chat-7b-v3-3&quot;, \
     &quot;prompt&quot;: &quot;What is Deep Learning?&quot;, \
     &quot;max_tokens&quot;: 32, &quot;temperature&quot;: 0}&#39;
</pre></div>
</div>
<p>vLLM service generates text for the input prompt. Here is the expected result
from vllm:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;generated_text&quot;</span><span class="p">:</span><span class="s2">&quot;We have all heard the buzzword, but our understanding of it is still growing. It’s a sub-field of Machine Learning, and it’s the cornerstone of today’s Machine Learning breakthroughs.</span><span class="se">\n\n</span><span class="s2">Deep Learning makes machines act more like humans through their ability to generalize from very large&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p><strong>NOTE</strong>: After launch the vLLM, it takes few minutes for vLLM server to load
LLM model and warm up.</p>
</div>
<input id="sd-tab-item-15" name="sd-tab-set-7" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-15">
TGI</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl http://${host_ip}:9009/generate \
  -X POST \
  -d &#39;{&quot;inputs&quot;:&quot;What is Deep Learning?&quot;, \
     &quot;parameters&quot;:{&quot;max_new_tokens&quot;:17, &quot;do_sample&quot;: true}}&#39; \
  -H &#39;Content-Type: application/json&#39;

</pre></div>
</div>
<p>TGI service generate text for the input prompt. Here is the expected result from TGI:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;generated_text&quot;</span><span class="p">:</span><span class="s2">&quot;We have all heard the buzzword, but our understanding of it is still growing. It’s a sub-field of Machine Learning, and it’s the cornerstone of today’s Machine Learning breakthroughs.</span><span class="se">\n\n</span><span class="s2">Deep Learning makes machines act more like humans through their ability to generalize from very large&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p><strong>NOTE</strong>: After launch the TGI, it takes few minutes for TGI server to load LLM model and warm up.</p>
</div>
</div>
<p>If you get</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span><span class="p">:</span> <span class="p">(</span><span class="mi">7</span><span class="p">)</span> <span class="n">Failed</span> <span class="n">to</span> <span class="n">connect</span> <span class="n">to</span> <span class="mf">100.81.104.168</span> <span class="n">port</span> <span class="mi">8008</span> <span class="n">after</span> <span class="mi">0</span> <span class="n">ms</span><span class="p">:</span> <span class="n">Connection</span> <span class="n">refused</span>

</pre></div>
</div>
<p>and the log shows model warm up, please wait for a while and try it later.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>2024-06-05T05:45:27.707509646Z 2024-06-05T05:45:27.707361Z  WARN text_generation_router: router/src/main.rs:357: `--revision` is not set
2024-06-05T05:45:27.707539740Z 2024-06-05T05:45:27.707379Z  WARN text_generation_router: router/src/main.rs:358: We strongly advise to set it to a known supported commit.
2024-06-05T05:45:27.852525522Z 2024-06-05T05:45:27.852437Z  INFO text_generation_router: router/src/main.rs:379: Serving revision bdd31cf498d13782cc7497cba5896996ce429f91 of model Intel/neural-chat-7b-v3-3
2024-06-05T05:45:27.867833811Z 2024-06-05T05:45:27.867759Z  INFO text_generation_router: router/src/main.rs:221: Warming up model

</pre></div>
</div>
</section>
<section id="llm-microservice">
<h3>LLM Microservice<a class="headerlink" href="#llm-microservice" title="Link to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl http://${host_ip}:9000/v1/chat/completions\
  -X POST \
  -d &#39;{&quot;query&quot;:&quot;What is Deep Learning?&quot;,&quot;max_new_tokens&quot;:17,&quot;top_k&quot;:10,&quot;top_p&quot;:0.95,\
     &quot;typical_p&quot;:0.95,&quot;temperature&quot;:0.01,&quot;repetition_penalty&quot;:1.03,&quot;streaming&quot;:true}&#39; \
  -H &#39;Content-Type: application/json&#39;

</pre></div>
</div>
<p>You will get generated text from LLM:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;Deep&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; learning&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; is&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; a&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; subset&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; of&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; machine&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; learning&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; that&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; uses&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; algorithms&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; to&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; learn&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; from&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; data&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="p">[</span><span class="n">DONE</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id1">
<h3>MegaService<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>curl http://${host_ip}:8888/v1/chatqna -H &quot;Content-Type: application/json&quot; -d &#39;{
     &quot;model&quot;: &quot;Intel/neural-chat-7b-v3-3&quot;,
     &quot;messages&quot;: &quot;What is the revenue of Nike in 2023?&quot;
     }&#39;

</pre></div>
</div>
<p>Here is the output for your reference:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;An&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;swer&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;:&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; In&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; fiscal&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; &#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;2&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;0&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;2&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;3&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;,&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; N&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;I&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;KE&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;,&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; Inc&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;.&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; achieved&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; record&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; Rev&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;en&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;ues&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; of&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; $&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;5&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;1&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;.&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;2&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39; billion&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;.&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="sa">b</span><span class="s1">&#39;&lt;/s&gt;&#39;</span>
<span class="n">data</span><span class="p">:</span> <span class="p">[</span><span class="n">DONE</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="check-docker-container-log">
<h2>Check docker container log<a class="headerlink" href="#check-docker-container-log" title="Link to this heading">¶</a></h2>
<p>Check the log of container by:</p>
<p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">logs</span> <span class="pre">&lt;CONTAINER</span> <span class="pre">ID&gt;</span> <span class="pre">-t</span></code></p>
<p>Check the log by  <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">logs</span> <span class="pre">f7a08f9867f9</span> <span class="pre">-t</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2024</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">05</span><span class="n">T01</span><span class="p">:</span><span class="mi">30</span><span class="p">:</span><span class="mf">30.695934928</span><span class="n">Z</span> <span class="n">error</span><span class="p">:</span> <span class="n">a</span> <span class="n">value</span> <span class="ow">is</span> <span class="n">required</span> <span class="k">for</span> <span class="s1">&#39;--model-id &lt;MODEL_ID&gt;&#39;</span> <span class="n">but</span> <span class="n">none</span> <span class="n">was</span> <span class="n">supplied</span>
<span class="mi">2024</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">05</span><span class="n">T01</span><span class="p">:</span><span class="mi">30</span><span class="p">:</span><span class="mf">30.697123534</span><span class="n">Z</span>
<span class="mi">2024</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">05</span><span class="n">T01</span><span class="p">:</span><span class="mi">30</span><span class="p">:</span><span class="mf">30.697148330</span><span class="n">Z</span> <span class="n">For</span> <span class="n">more</span> <span class="n">information</span><span class="p">,</span> <span class="k">try</span> <span class="s1">&#39;--help&#39;</span><span class="o">.</span>

</pre></div>
</div>
<p>The log indicates the <code class="docutils literal notranslate"><span class="pre">MODEL_ID</span></code> is not set.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-16" name="sd-tab-set-8" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-16">
vllm</label><div class="sd-tab-content docutils">
<p>View the docker input parameters in  <code class="docutils literal notranslate"><span class="pre">./ChatQnA/docker_compose/intel/cpu/xeon/compose_vllm.yaml</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>vllm_service:
    image: ${REGISTRY:-opea}/vllm:${TAG:-latest}
    container_name: vllm-service
    ports:
      - &quot;9009:80&quot;
    volumes:
      - &quot;./data:/data&quot;
    shm_size: 128g
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      LLM_MODEL_ID: ${LLM_MODEL_ID}
    command: --model $LLM_MODEL_ID --host 0.0.0.0 --port 80

</pre></div>
</div>
</div>
<input id="sd-tab-item-17" name="sd-tab-set-8" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-17">
TGI</label><div class="sd-tab-content docutils">
<p>View the docker input parameters in  <code class="docutils literal notranslate"><span class="pre">./ChatQnA/docker_compose/intel/cpu/xeon/compose.yaml</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> tgi-service:
    image: ghcr.io/huggingface/text-generation-inference:sha-e4201f4-intel-cpu
    container_name: tgi-service
    ports:
      - &quot;9009:80&quot;
    volumes:
      - &quot;./data:/data&quot;
    shm_size: 1g
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HF_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      HF_HUB_DISABLE_PROGRESS_BARS: 1
      HF_HUB_ENABLE_HF_TRANSFER: 0
    command: --model-id ${LLM_MODEL_ID} --cuda-graphs 0

</pre></div>
</div>
</div>
</div>
<p>The input <code class="docutils literal notranslate"><span class="pre">MODEL_ID</span></code> is  <code class="docutils literal notranslate"><span class="pre">${LLM_MODEL_ID}</span></code></p>
<p>Check environment variable  <code class="docutils literal notranslate"><span class="pre">LLM_MODEL_ID</span></code>  is set correctly, spelled correctly.
Set the <code class="docutils literal notranslate"><span class="pre">LLM_MODEL_ID</span></code> then restart the containers.</p>
<p>Also you can check overall logs with the following command, where the
compose.yaml is the mega service docker-compose configuration file.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-18" name="sd-tab-set-9" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-18">
vllm</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">compose</span> <span class="o">-</span><span class="n">f</span> <span class="o">./</span><span class="n">docker_compose</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">cpu</span><span class="o">/</span><span class="n">xeon</span><span class="o">/</span><span class="n">compose_vllm</span><span class="o">.</span><span class="n">yaml</span> <span class="n">logs</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-19" name="sd-tab-set-9" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-19">
TGI</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">compose</span> <span class="o">-</span><span class="n">f</span> <span class="o">./</span><span class="n">docker_compose</span><span class="o">/</span><span class="n">intel</span><span class="o">/</span><span class="n">cpu</span><span class="o">/</span><span class="n">xeon</span><span class="o">/</span><span class="n">compose</span><span class="o">.</span><span class="n">yaml</span> <span class="n">logs</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="launch-ui">
<h2>Launch UI<a class="headerlink" href="#launch-ui" title="Link to this heading">¶</a></h2>
<section id="basic-ui">
<h3>Basic UI<a class="headerlink" href="#basic-ui" title="Link to this heading">¶</a></h3>
<p>To access the frontend, open the following URL in your browser: http://{host_ip}:5173. By default, the UI runs on port 5173 internally. If you prefer to use a different host port to access the frontend, you can modify the port mapping in the compose.yaml file as shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">chaqna</span><span class="o">-</span><span class="n">xeon</span><span class="o">-</span><span class="n">ui</span><span class="o">-</span><span class="n">server</span><span class="p">:</span>
    <span class="n">image</span><span class="p">:</span> <span class="n">opea</span><span class="o">/</span><span class="n">chatqna</span><span class="o">-</span><span class="n">ui</span><span class="p">:</span><span class="n">latest</span>
    <span class="o">...</span>
    <span class="n">ports</span><span class="p">:</span>
      <span class="o">-</span> <span class="s2">&quot;80:5173&quot;</span>
</pre></div>
</div>
</section>
<section id="conversational-ui">
<h3>Conversational UI<a class="headerlink" href="#conversational-ui" title="Link to this heading">¶</a></h3>
<p>To access the Conversational UI (react based) frontend, modify the UI service in the compose.yaml file. Replace chaqna-xeon-ui-server service with the chatqna-xeon-conversation-ui-server service as per the config below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>chaqna-xeon-conversation-ui-server:
  image: opea/chatqna-conversation-ui:latest
  container_name: chatqna-xeon-conversation-ui-server
  environment:
    - APP_BACKEND_SERVICE_ENDPOINT=${BACKEND_SERVICE_ENDPOINT}
    - APP_DATA_PREP_SERVICE_URL=${DATAPREP_SERVICE_ENDPOINT}
  ports:
    - &quot;5174:80&quot;
  depends_on:
    - chaqna-xeon-backend-server
  ipc: host
  restart: always
</pre></div>
</div>
<p>Once the services are up, open the following URL in your browser: http://{host_ip}:5174. By default, the UI runs on port 80 internally. If you prefer to use a different host port to access the frontend, you can modify the port mapping in the compose.yaml file as shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">chaqna</span><span class="o">-</span><span class="n">xeon</span><span class="o">-</span><span class="n">conversation</span><span class="o">-</span><span class="n">ui</span><span class="o">-</span><span class="n">server</span><span class="p">:</span>
    <span class="n">image</span><span class="p">:</span> <span class="n">opea</span><span class="o">/</span><span class="n">chatqna</span><span class="o">-</span><span class="n">conversation</span><span class="o">-</span><span class="n">ui</span><span class="p">:</span><span class="n">latest</span>
    <span class="o">...</span>
    <span class="n">ports</span><span class="p">:</span>
      <span class="o">-</span> <span class="s2">&quot;80:80&quot;</span>
</pre></div>
</div>
</section>
<section id="stop-the-services">
<h3>Stop the services<a class="headerlink" href="#stop-the-services" title="Link to this heading">¶</a></h3>
<p>Once you are done with the entire pipeline and wish to stop and remove all the containers, use the command below:</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-20" name="sd-tab-set-10" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="vllm" for="sd-tab-item-20">
vllm</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">compose</span> <span class="o">-</span><span class="n">f</span> <span class="n">compose_vllm</span><span class="o">.</span><span class="n">yaml</span> <span class="n">down</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-21" name="sd-tab-set-10" type="radio">
<label class="sd-tab-label" data-sync-group="tab" data-sync-id="TGI" for="sd-tab-item-21">
TGI</label><div class="sd-tab-content docutils">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">compose</span> <span class="o">-</span><span class="n">f</span> <span class="n">compose</span><span class="o">.</span><span class="n">yaml</span> <span class="n">down</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="usage.html" class="btn btn-neutral float-left" title="Build MegaService of CodeGen on Xeon" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Alberto Gallegos.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-3QH5804YP8"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-3QH5804YP8', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>